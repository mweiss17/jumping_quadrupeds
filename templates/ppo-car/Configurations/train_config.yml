# env_name (str): Environment name, used for logging
env_name:  "Duckietown-small_loop-v0" #CarRacing-v0 #

# verbose (bool): should PPO print verbose debugging messages to stdout?
verbose: False

# steps_per_epoch (int): Number of steps of interaction (state-action pairs for the agent and the environment
#  in each epoch.
steps_per_epoch: 4000

# epochs (int): Number of epochs of interaction (equivalent to number of policy updates) to perform.
epochs: 250

# gamma (float): Discount factor. (Always between 0 and 1.)
gamma: 0.99

# clip_ratio (float): Hyperparameter for clipping in the policy objective.
#  Roughly: how far can the new policy go from the old policy while
#  still profiting (improving the objective function)? The new policy
#  can still go farther than the clip_ratio says, but it doesn't help
#  on the objective anymore. (Usually small, 0.1 to 0.3.) Typically
#  denoted by :math:`\epsilon`.
clip_ratio: 0.2

# pi_lr (float): Learning rate for policy optimizer.
pi_lr: 3.0e-4

# vf_lr (float): Learning rate for value function optimizer.
vf_lr: 1.0e-3

# train_pi_iters (int): Maximum number of gradient descent steps to take on policy loss per epoch. (Early stopping
#  may cause optimizer to take fewer than this.)
train_pi_iters: 4

# train_v_iters (int): Number of gradient descent steps to take on value function per epoch.
train_v_iters: 4

# lam (float): Lambda for GAE-Lambda. (Always between 0 and 1, close to 1.)
lam: 0.97

# max_ep_len (int): Maximum length of trajectory / episode / rollout. This is only used by some variable-length
#  episode environments
max_ep_len: 1000

# target_kl (float): Roughly what KL divergence we think is appropriate between new and old policies after an
#  update. This will get used for early stopping. (Usually small, 0.01 or 0.05.)
target_kl: 0.05

# save_freq (int): save policy every n epochs
save_freq: 10

# log_ep_freq (int): log rewards every n episodes
log_ep_freq: 1

# rew_smooth_len (int): rewards are accumulated in a ring buffer and this determines the size
rew_smooth_len: 10

# seed (int): random seed
seed: 1

use_wandb: True

log_std: 0.5
#save_transitions: 500000

# in case you want to start from  a checkpoint, set these variables (or pass them through command line)
# ppo_checkpoint: experiments/ppo/Weights/ac-40.pt
# pi_optim_checkpoint: experiments/ppo/Weights/pi-optim-40.pt
# vf_optim_checkpoint: experiments/ppo/Weights/vf-optim-40.pt

# Sweep: Oct 19, 2021
#  python3 scripts/salvo.py --- scripts/rl-tests/03-ppo-car.py ~/scratch-new/experiments/ppo-finetune-{job_idx} --inherit templates/ppo-car --config._wandb_run_name ppo-finetune-{job_idx} --config._wandb_group ppo-finetune-sweep --config.clip_ratio {clip} --config.pi_lr {pi_lr} --config.vf_lr {vf_lr} --config.target_kl {target_kl} --- jumping_quadrupeds/hyper/random_search.py -d pi_lr~log_uniform[0.00001,0.001]+vf_lr~log_uniform[0.00001,0.001]+clip~log_uniform[0.05,0.4]+target_kl~uniform[0.005,0.02] -n 50