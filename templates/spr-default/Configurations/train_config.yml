# env_name (str): Environment name, used for logging
env_name:  gym-CarRacing-v0 #atari-AirRaid-v0 #"gym-Duckietown-small_loop-v0" #CarRacing-v0 # dm-quadruped_walk

# verbose (bool): should PPO print verbose debugging messages to stdout?
verbose: False

# max_ep_len (int): Maximum length of trajectory / episode / rollout. This is only used by some variable-length
#  episode environments
max_ep_len: 1000

total_steps: 1000000

# save_freq (int): save policy every n epochs
save_freq: 10

# log_ep_freq (int): log rewards every n episodes
log_ep_freq: 1

# rew_smooth_len (int): rewards are accumulated in a ring buffer and this determines the size
rew_smooth_len: 10

# seed (int): random seed
seed: 1

use_wandb: True

log_std: -.75 #-1.2
#save_transitions: 500000

# in case you want to start from  a checkpoint, set these variables (or pass them through command line)
# ppo_checkpoint: experiments/ppo/Weights/ac-40.pt
# pi_optim_checkpoint: experiments/ppo/Weights/pi-optim-40.pt
# vf_optim_checkpoint: experiments/ppo/Weights/vf-optim-40.pt
agent:
  name: spr
#  kwargs:
#    eps_init:  1. # defaults from SPR paper
#    eps_final: 0.
#    eps_eval:  0.001

model:
  kwargs:
    dueling: 1
    noisy_nets: 1
    noisy_nets_std: 0.5
    imagesize: 84
    jumps: 5
    dynamics_blocks: 0
    spr: 1
    momentum_encoder: 1
    shared_encoder: 0
    local_spr: 0
    global_spr: 1
    distributional: 1
    renormalize: 1
    norm_type: 'bn' # choices=["bn", "ln", "in", "none"]
    augmentation: ["shift", "intensity"] # choices=["none", "rrc", "affine", "crop", "blur", "shift", "intensity"],
    q_l1_type: ["value", "advantage"] # choices=["noisy", "value", "advantage", "relu"]
    dropout: 0. # prob of dropout in conv net
    time_offset: 0
    aug_prob: 1.
    target_augmentation: 1.
    eval_augmentation: 0.
    classifier: "q_l1" # choices=["mlp", "bilinear", "q_l1", "q_l2", "none"],
    final_classifier: "linear" # style of nce classifier choices=["mlp", "linear", "none"], h
    momentum_tau: 0.01
    dqn_hidden_size: 256
    model_rl: 0.
    residual_tm: 0.

buffer:
  on_policy: False
  kwargs:
    max_size:  1000000
    nstep: 3
    batch_size: 64
    fetch_every: 4000
    num_workers: 0
    save_snapshot: True
    discount: 0.99
    gae_lambda: 0.97


optim:
  kwargs:
    eps: 0.00015

algo:
  kwargs:
      min_steps_learn: 2000
      n_step_return: 100000
      batch_size: 32
      learning_rate: 0.0001
      replay_ratio: 64
      target_update_interval: 1
      target_update_tau: 1.
      eps_steps: 2001
      clip_grad_norm: 10.
      pri_alpha: 0.5
      pri_beta_steps: 10e4
      model_rl_weight: 0.
      reward_loss_weight: 0
      model_spr_weight: 5.
      t0_spr_loss_weight: 0.
      time_offset: 0
      distributional: 1
      delta_clip: 1.
      prioritized_replay: 1

