{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b917379f-c28c-4d7c-9901-72ee87a16e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: ipywidgets in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (7.6.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipywidgets) (7.29.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: decorator in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (58.0.4)\n",
      "Requirement already satisfied: pygments in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: entrypoints in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.2.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.8)\n",
      "Requirement already satisfied: jinja2 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: nbconvert in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.13.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: testpath in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.10)\n",
      "Requirement already satisfied: bleach in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: webencodings in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Requirement already satisfied: dill in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (0.3.4)\n",
      "Requirement already satisfied: torchmeta in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: torchvision<0.11.0,>=0.5.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (0.10.1)\n",
      "Requirement already satisfied: h5py in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (3.4.0)\n",
      "Requirement already satisfied: requests in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (2.26.0)\n",
      "Requirement already satisfied: ordered-set in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (4.1.0)\n",
      "Requirement already satisfied: tqdm>=4.0.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (4.62.3)\n",
      "Requirement already satisfied: torch<1.10.0,>=1.4.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (1.21.2)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torchmeta) (8.3.2)\n",
      "Requirement already satisfied: typing-extensions in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from torch<1.10.0,>=1.4.0->torchmeta) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from requests->torchmeta) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from requests->torchmeta) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from requests->torchmeta) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/martin/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages (from requests->torchmeta) (3.2)\n",
      "fatal: destination path 'vit-pytorch' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "# !pip install vit_pytorch\n",
    "!pip install ipywidgets\n",
    "!pip install dill\n",
    "!pip install torchmeta\n",
    "!git clone https://github.com/lucidrains/vit-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e1921-5679-45bc-ad59-acfd14b2f317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4b8114-d3a7-4828-9e13-32852d7c5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "import math\n",
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchmeta.datasets.helpers import miniimagenet\n",
    "from torchmeta.utils.data import NonEpisodicWrapper\n",
    "\n",
    "from jumping_quadrupeds.eth.dataset import Hdf5ImgDataset\n",
    "from jumping_quadrupeds.utils import common_img_transforms, abs_path\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d12d797-ce32-4ca1-a023-275b57fd5f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18f0831-17f5-44b3-8063-220195314d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    batch, seq_length, _ = t.shape\n",
    "    return torch.zeros((batch, seq_length)).float().uniform_(0, 1) < prob\n",
    "\n",
    "def get_mask_subset_with_prob(patched_input, prob):\n",
    "    batch, seq_len, _, device = *patched_input.shape, patched_input.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len), device=device)\n",
    "    new_mask.scatter_(1, sampled_indices, 1)\n",
    "    return new_mask.bool()\n",
    "\n",
    "\n",
    "def visualize(pred_pixel_values, patches, sample_id=0):\n",
    "    masked_patch_pred = pred_pixel_values[sample_id].detach().cpu()\n",
    "    masked_patch_true = patches[sample_id].cpu()\n",
    "\n",
    "    pred_patches = rearrange(masked_patch_pred, 'p (h w c) -> p c h w', c=3, h=12)\n",
    "    gt_patches = rearrange(masked_patch_true, 'p (h w c) -> p c h w', c=3, h=12)\n",
    "\n",
    "    pred_recons = gt_patches.clone()\n",
    "    pred_w_mask = gt_patches.clone()\n",
    "    pred_recons[masked_indices[sample_id]] = pred_patches\n",
    "    pred_w_mask[masked_indices[sample_id]] = 0.\n",
    "\n",
    "    imshow(torchvision.utils.make_grid(images[sample_id].cpu()))\n",
    "    imshow(torchvision.utils.make_grid(pred_recons, nrow=7))\n",
    "    imshow(torchvision.utils.make_grid(pred_w_mask, nrow=7))\n",
    "    imshow(torchvision.utils.make_grid(gt_patches, nrow=7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89665e76-1cd0-4b7d-8f2e-4acacfdac706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7b9ad4-af6a-49e1-b074-f16cb1eeef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        encoder,\n",
    "        decoder_dim,\n",
    "        masking_ratio = 0.75,\n",
    "        decoder_depth = 1,\n",
    "        decoder_heads = 8,\n",
    "        decoder_dim_head = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert masking_ratio > 0 and masking_ratio < 1, 'masking ratio must be kept between 0 and 1'\n",
    "        self.masking_ratio = masking_ratio\n",
    "\n",
    "        # extract some hyperparameters and functions from encoder (vision transformer to be trained)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        num_patches, encoder_dim = encoder.pos_embedding.shape[-2:]\n",
    "        self.to_patch, self.patch_to_emb = encoder.to_patch_embedding[:2]\n",
    "        pixel_values_per_patch = self.patch_to_emb.weight.shape[-1]\n",
    "        # decoder parameters\n",
    "\n",
    "        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
    "        self.mask_token = nn.Parameter(torch.randn(decoder_dim))\n",
    "        self.decoder = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
    "        self.decoder_pos_emb = nn.Embedding(num_patches, decoder_dim)\n",
    "        self.to_pixels = nn.Linear(decoder_dim, pixel_values_per_patch)\n",
    "\n",
    "    def forward(self, img):\n",
    "        device = img.device\n",
    "\n",
    "        # get patches\n",
    "        patches = self.to_patch(img)\n",
    "        batch, num_patches, *_ = patches.shape\n",
    "\n",
    "        # patch to encoder tokens and add positions\n",
    "        tokens = self.patch_to_emb(patches)\n",
    "        tokens = tokens + self.encoder.pos_embedding[:, 1:(num_patches + 1)]\n",
    "\n",
    "        # calculate of patches needed to be masked, and get random indices, dividing it up for mask vs unmasked\n",
    "        num_masked = int(self.masking_ratio * num_patches)\n",
    "        rand_indices = torch.rand(batch, num_patches, device = device).argsort(dim = -1)\n",
    "        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "        # get the unmasked tokens to be encoded\n",
    "        batch_range = torch.arange(batch, device = device)[:, None]\n",
    "        tokens = tokens[batch_range, unmasked_indices]\n",
    "\n",
    "        # get the patches to be masked for the final reconstruction loss\n",
    "        masked_patches = patches[batch_range, masked_indices]\n",
    "\n",
    "        # attend with vision transformer\n",
    "        encoded_tokens = self.encoder.transformer(tokens)\n",
    "\n",
    "        # project encoder to decoder dimensions, if they are not equal - the paper says you can get away with a smaller dimension for decoder\n",
    "        decoder_tokens = self.enc_to_dec(encoded_tokens)\n",
    "\n",
    "        # reapply decoder position embedding to unmasked tokens\n",
    "        decoder_tokens = decoder_tokens + self.decoder_pos_emb(unmasked_indices)\n",
    "\n",
    "        # repeat mask tokens for number of masked, and add the positions using the masked indices derived above\n",
    "        mask_tokens = repeat(self.mask_token, 'd -> b n d', b = batch, n = num_masked)\n",
    "        mask_tokens = mask_tokens + self.decoder_pos_emb(masked_indices)\n",
    "\n",
    "        # concat the masked tokens to the decoder tokens and attend with decoder\n",
    "        decoder_tokens = torch.cat((mask_tokens, decoder_tokens), dim = 1)\n",
    "        decoded_tokens = self.decoder(decoder_tokens)\n",
    "\n",
    "        # splice out the mask tokens and project to pixel values\n",
    "        mask_tokens = decoded_tokens[:, :num_masked]\n",
    "        pred_pixel_values = self.to_pixels(mask_tokens)\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "        recon_loss = F.mse_loss(pred_pixel_values, masked_patches)\n",
    "        return recon_loss, pred_pixel_values, masked_indices, unmasked_indices, patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d6c668-a708-4e13-9d5b-f7ff86f91229",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 84\n",
    "PATCH_SIZE = 12 # 32\n",
    "NUM_PATCHES = 84//12\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "vit = ViT(\n",
    "    image_size=IMAGE_SIZE, #256\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=1000,\n",
    "    dim=1024,\n",
    "    depth=2,\n",
    "    heads=8,\n",
    "    mlp_dim=512, #2048\n",
    "    # dropout=0.1,\n",
    "    # emb_dropout=0.1\n",
    ").to(device)\n",
    "mae = MAE(\n",
    "    encoder = vit,\n",
    "    masking_ratio = 0.75,   # the paper recommended 75% masked patches\n",
    "    decoder_dim = 128,      # paper showed good results with just 512\n",
    "    decoder_depth = 1       # anywhere from 1 to 8\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(mae.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d101f67-164f-4899-9047-0fdac39356ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = common_img_transforms(size=IMAGE_SIZE)\n",
    "\n",
    "ds_path = \"../random-rollouts-50k/CarRacing-v0_rollouts_e256_t200.hdf5\"\n",
    "train_dataset = Hdf5ImgDataset(ds_path, transform=transform, flat=True)\n",
    "sampler = RandomSampler(train_dataset)\n",
    "dataloader = DataLoader(train_dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fde71-be90-40d5-8bbe-df0b4450283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.9119821190834045\n",
      "step: 10, loss: 0.6261051297187805\n",
      "step: 20, loss: 0.4972563683986664\n",
      "step: 30, loss: 0.4269775152206421\n",
      "step: 40, loss: 0.37343186140060425\n",
      "step: 50, loss: 0.32535064220428467\n",
      "step: 60, loss: 0.2833450436592102\n",
      "step: 70, loss: 0.24848751723766327\n",
      "step: 80, loss: 0.21836702525615692\n",
      "step: 90, loss: 0.1918756514787674\n",
      "step: 100, loss: 0.16875161230564117\n",
      "step: 110, loss: 0.14733588695526123\n",
      "step: 120, loss: 0.12789905071258545\n",
      "step: 130, loss: 0.11030003428459167\n",
      "step: 140, loss: 0.09541065990924835\n",
      "step: 150, loss: 0.08265388756990433\n",
      "step: 160, loss: 0.07206447422504425\n",
      "step: 170, loss: 0.06329181045293808\n",
      "step: 180, loss: 0.05591508001089096\n",
      "step: 190, loss: 0.05008135363459587\n",
      "step: 200, loss: 0.045471079647541046\n",
      "step: 210, loss: 0.042090222239494324\n",
      "step: 220, loss: 0.04019186273217201\n",
      "step: 230, loss: 0.03875730559229851\n",
      "step: 240, loss: 0.03759666159749031\n",
      "step: 250, loss: 0.036513734608888626\n",
      "step: 260, loss: 0.03363674879074097\n",
      "step: 270, loss: 0.03144634887576103\n",
      "step: 280, loss: 0.029577933251857758\n",
      "step: 290, loss: 0.027229901403188705\n",
      "step: 300, loss: 0.02568293735384941\n",
      "step: 310, loss: 0.024069227278232574\n",
      "step: 320, loss: 0.02238639071583748\n",
      "step: 330, loss: 0.021355973556637764\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for step, images in enumerate(dataloader_iter):\n",
    "        images = images.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loss, pred_pixel_values, masked_indices, unmasked_indices, patches = mae(images)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"step: {step}, loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a3aed9-3d35-4b4a-9b24-74a92e86c195",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23698/3243261054.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_pixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize' is not defined"
     ]
    }
   ],
   "source": [
    "visualize(pred_pixel_values, patches, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb80bab2-2d59-4760-8fe4-94707a0dd041",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6371/2954795542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \"\"\"\n\u001b[1;32m    377\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backend_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jumping_quadrupeds/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# only call close('all') if any to close\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# close triggers gc.collect, which can be slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "imgs = x[0]\n",
    "for i in range(imgs.shape[0]):\n",
    "    plt.show(imgs[i].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08d80b1e-df48-4eec-8d58-b0cec9797f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = miniimagenet('data', ways=5, shots=5, meta_train=True, download=True)\n",
    "dataset = NonEpisodicWrapper(dataset)\n",
    "dataloader = iter(cycle(DataLoader(dataset, batch_size=128, shuffle=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2496e5b6-4a4c-48f8-bd74-e39642f03ff9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6148/3029450315.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_pixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmasked_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for step, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        loss, pred_pixel_values, masked_indices, unmasked_indices, patches = mae(images)\n",
    "        opt.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"step: {step}, loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jumping_quadrupeds",
   "language": "python",
   "name": "jumping_quadrupeds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
